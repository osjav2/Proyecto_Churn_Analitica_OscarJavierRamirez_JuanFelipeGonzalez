# -*- coding: utf-8 -*-
"""evaluation.py

Automatically generated by Colab.

Original file is located a
    https://colab.research.google.com/drive/18RN4bSQ0ScT3p9Ymh9rqugH_Hn3rjAD3
"""

# evaluation.py
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score
from sklearn.inspection import permutation_importance
import numpy as np


# --- 1. Reporte de Métricas Individuales ---
def print_evaluation_report(model, X_test, y_test, model_name="Modelo"):
    """
    Imprime el reporte de clasificación (con foco en Churn/Clase 1) y ROC-AUC Score.
    """
    y_pred = model.predict(X_test)

    print(f"\n--- Resultados del {model_name.upper()} en el Conjunto de Prueba ---")

    # Reporte de Clasificación (Clase 1: Churn, Clase 0: No Churn)
    print(classification_report(y_test, y_pred, target_names=['No Churn (0)', 'Churn (1)']))

    # ROC-AUC Score (Fiabilidad Global)
    roc_auc = None
    try:
        # Se asume que la probabilidad de la Clase 1 (Churn) está en la columna [:, 1]
        y_prob = model.predict_proba(X_test)[:, 1]
        roc_auc = roc_auc_score(y_test, y_prob)
        print(f"ROC-AUC Score: {roc_auc:.4f}")
    except AttributeError:
        print("Advertencia: El modelo no soporta predict_proba para ROC-AUC.")
    except IndexError:
        print("Advertencia: No se pudo calcular ROC-AUC. Revisar orden de clases.")

    # Gráfica de Matriz de Confusión
    plot_confusion_matrix(y_test, y_pred, model_name)


# --- 2. Matriz de Confusión (Visual) ---
def plot_confusion_matrix(y_true, y_pred, model_name):
    """
    Grafica la matriz de confusión.
    """
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['No Churn (0)', 'Churn (1)'], yticklabels=['No Churn (0)', 'Churn (1)'])
    plt.title(f'Matriz de Confusión - {model_name}')
    plt.ylabel('Clase Real (True Class)')
    plt.xlabel('Clase Predicha (Predicted Class)')
    plt.show()


# --- 3. Importancia de Variables (Gini Importance) ---
def plot_feature_importance(model, X_test, top_n=10, model_name="Modelo"):
    """
    CORREGIDO: Extrae y grafica la Importancia de Variables (Gini o similar) para modelos basados en árboles
    dentro de un Pipeline Sklearn.
    """
    try:
        # 1. Extraer los componentes clave del Pipeline
        classifier = model.named_steps['classifier']
        preprocessor = model.named_steps['preprocessor']

        # 2. Obtener los nombres de las variables después del preprocesamiento (Importante para OHE)
        feature_names = preprocessor.get_feature_names_out()

        # 3. Obtener las importancias del clasificador
        if hasattr(classifier, 'feature_importances_'):
            importances = classifier.feature_importances_
        elif hasattr(classifier, 'estimator_') and hasattr(classifier.estimator_, 'feature_importances_'):
            # Para Bagging que usa un estimador base con feature_importances_
            importances = classifier.estimator_.feature_importances_
        else:
            print(
                f"\nAdvertencia: El clasificador {type(classifier).__name__} no tiene el atributo 'feature_importances_'.")
            return

        feature_importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': importances
        }).sort_values('importance', ascending=False)

        print(f"\n--- Top {top_n} Variables más Importantes (Gini) de {model_name} ---")
        display(feature_importance_df.head(top_n))

        plt.figure(figsize=(12, 6))
        sns.barplot(x='importance', y='feature', data=feature_importance_df.head(top_n), palette='viridis',
                    hue='feature', legend=False)
        plt.title(f'Top {top_n} Palancas de Negocio ({model_name})')
        plt.xlabel('Puntuación de Impacto (Gini Importance)')
        plt.ylabel('Variable de Negocio')
        plt.tight_layout()
        plt.show()

    except Exception as e:
        print(f"Ocurrió un error al intentar graficar la importancia: {e}")
        print(
            "Asegúrese de que el modelo sea un Pipeline Sklearn válido y que el clasificador soporte feature_importances_.")


# --- 4. Comparativa de Modelos (Tabla Final para Informe) ---
def compare_models(results_dict):
    """
    Crea una tabla comparativa de las métricas clave, enfocada en la CLASE CHURN (1).
    """
    comparison_list = []

    for name, result in results_dict.items():
        y_test = result['y_test']
        X_test = result['X_test']
        model = result['model']
        y_pred = model.predict(X_test)

        # Calcular ROC-AUC Score
        roc_auc = None
        try:
            # Se asume Clase 1 (Churn)
            y_prob = model.predict_proba(X_test)[:, 1]
            roc_auc = roc_auc_score(y_test, y_prob)
        except:
            pass

        # classification_report a diccionario para fácil extracción
        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)

        # Clase '1' (Churn) puede ser str o int
        churn_report = report.get(1) or report.get('1', {})

        # Extraer métricas de la CLASE CHURN (1)
        comparison_list.append({
            'Modelo': name,
            'Accuracy': report.get('accuracy', 0),
            'Recall (Churn)': churn_report.get('recall', 0),
            'Precision (Churn)': churn_report.get('precision', 0),
            'F1-Score (Churn)': churn_report.get('f1-score', 0),
            'ROC-AUC Score': roc_auc
        })

    df_comparison = pd.DataFrame(comparison_list).set_index('Modelo')
    print("\n--- COMPARATIVA FINAL DE MODELOS (Foco en Churn) ---")
    display(df_comparison.round(4))

    return df_comparison